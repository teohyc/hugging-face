{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99a1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline, StableDiffusionDepth2ImgPipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cadd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ee9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "\n",
    "init_image = download_image(img_url).resize((img_height, img_width))\n",
    "mask_image = download_image(mask_url).resize((img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e01eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#encode image\n",
    "init_image_tensor = torch.from_numpy(np.array(init_image).transpose(2, 0, 1)).float() / 255.0 #[0, 1]\n",
    "init_image_tensor = 2.0 * init_image_tensor - 1.0 # [-1, 1]\n",
    "init_image_tensor = init_image_tensor.unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ab4b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#inpainting\n",
    "\n",
    "#resize mask image\n",
    "mask_image_latent_size = mask_image.resize((64, 64))\n",
    "mask_image_latent_size = torch.tensor((np.array(mask_image_latent_size)[...,0] > 5).astype(np.float32)) #grab everything from the first channel which everything brighter than 5 as the mask\n",
    "\n",
    "plt.imshow(mask_image_latent_size.numpy(), cmap=\"gray\")\n",
    "\n",
    "mask_image_latent_size = mask_image_latent_size.to(device)\n",
    "mask_image_latent_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27f81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  7.55it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 176.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#load pipeline\u001b[39;00m\n\u001b[32m      2\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mstabilityai/stable-diffusion-2-1-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipe = \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#encode\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:541\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     module.to(device=device)\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    544\u001b[39m     module.dtype == torch.float16\n\u001b[32m    545\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    546\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    547\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    548\u001b[39m ):\n\u001b[32m    549\u001b[39m     logger.warning(\n\u001b[32m    550\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    551\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4110\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4109\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\hugging face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 176.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#load pipeline\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id).to(device)\n",
    "\n",
    "#encode\n",
    "with torch.no_grad():\n",
    "    init_image_latents = pipe.vae.encode(init_image_tensor).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "\n",
    "#denoising loop\n",
    "guidance_scale = 8\n",
    "num_inference_steps = 20\n",
    "prompt = \"A small creepy doll, broken, cursed, sitting on a park bench\"\n",
    "negative_prompt = \"zoomed in, blurry, oversaturated, warped, lively, blessed, holy\"\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "text_embeddings = pipe._encode_prompt(prompt, device, 1, True, negative_prompt)\n",
    "\n",
    "#random starting point\n",
    "latents = torch.randn((1, 4, 64, 64), device=device, generator=generator)\n",
    "latents *= pipe.scheduler.init_noise_sigma\n",
    "\n",
    "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "for i, t in enumerate(pipe.scheduler.timesteps):\n",
    "\n",
    "    #expand\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    #scale\n",
    "    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    #predict\n",
    "    with torch.no_grad():\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
    "\n",
    "    #guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    #x_t-1\n",
    "    latents = pipe.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "    #inpainting\n",
    "    if i < len(pipe.scheduler.timesteps) - 1:\n",
    "\n",
    "        noise = torch.randn(init_image_latents.shape, generator=generator, device=device, dtype=torch.float32) \n",
    "        background = pipe.scheduler.add_noise(\n",
    "            init_image_latents, noise, torch.tensor([pipe.scheduler.timesteps[i + 1]])\n",
    "        )\n",
    "\n",
    "        latents = latents * mask_image_latent_size #white in the areas\n",
    "        background = background * (1 - mask_image_latent_size) #black in the areas\n",
    "\n",
    "        #combine generated and original image latents based on mask\n",
    "        latents += background\n",
    "\n",
    "    #decode\n",
    "    pipe.vae.to(torch.float16)\n",
    "    latents_norm = latents.to(torch.float16) / pipe.vae.config.scaling_factor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inpainted_image = pipe.vae.decode(latents_norm).sample.float()\n",
    "\n",
    "    inpainted_image = (inpainted_image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "    inpainted_image = (inpainted_image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "    inpainted_image = Image.fromarray(inpainted_image)\n",
    "\n",
    "    inpainted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64020a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpainting by pipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A small creepy doll, broken, cursed, sitting on a park bench\"\n",
    "image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n",
    "\n",
    "#view\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs[0].imshow(init_image)\n",
    "axs[0].set_title(init_image)\n",
    "axs[1].imshow(mask_image)\n",
    "axs[1].set_title(\"Mask\")\n",
    "axs[2].imshow(image)\n",
    "axs[2].set_title(\"Result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
